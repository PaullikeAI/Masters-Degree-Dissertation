{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e6fb19",
   "metadata": {},
   "source": [
    "### Pacman - Naive Bayes Shield + Firepits + Route Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbe7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tkinter as tk\n",
    "from tkinter import Canvas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils import resample\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73c645",
   "metadata": {},
   "source": [
    "###### Pacman Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dba98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Pacman Map\n",
    "# 1 = wall, 2 = food, 0 = empty space\n",
    "INITIAL_MAZE = [\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1],\n",
    "    [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
    "    [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "    [1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1],\n",
    "    [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1],\n",
    "    [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1],\n",
    "    [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "]\n",
    "\n",
    "# Firepit Maze\n",
    "FIREPIT_MAZE = [\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 4, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
    "    [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "    [1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1],\n",
    "    [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1],\n",
    "    [1, 2, 0, 5, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 5, 5, 1, 1, 1, 0, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 5, 5, 0, 1, 1, 1, 0, 1, 1, 1],\n",
    "    [1, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "    [1, 0, 5, 0, 0, 5, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 5, 0, 5, 5, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c00d6a",
   "metadata": {},
   "source": [
    "#### Utility Functions (Statistics, Time formating, Data importing/exporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c95ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_rewards(reward_per_episode, episodes_completed):\n",
    "    \"\"\"Returns the reward for last 100 episodes for each episode\"\"\"\n",
    "    sum_of_rewards = np.zeros(episodes_completed)\n",
    "    for i in range(episodes_completed):\n",
    "        # Rewards for last 100 episodes\n",
    "        sum_of_rewards[i] = np.sum(reward_per_episode[max(0, i - 100):(i+1)])\n",
    "\n",
    "    return sum_of_rewards\n",
    "\n",
    "def cumulative_reward(rewards_per_episode):\n",
    "    \"\"\"Create a list of the cumulative reward the agent has gained per episode.\"\"\"\n",
    "    cumulative_reward = [rewards_per_episode[0]]\n",
    "    for i in rewards_per_episode[1:]:\n",
    "        cumulative_reward.append(i + cumulative_reward[-1])\n",
    "        \n",
    "    return cumulative_reward\n",
    "\n",
    "def one_in_x(a_list, x):\n",
    "    \"\"\"Returns every one in x of a given list for purpose of smoothing reward graphs.\"\"\"\n",
    "    return a_list[::x]\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Formats time in seconds into hours/minutes/seconds and returns a string of the resulting time.\"\"\"\n",
    "    minutes = int(seconds // 60)\n",
    "    hours = int(minutes // 60)\n",
    "    minutes = minutes % 60\n",
    "    seconds = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours} hours, {minutes} minute{'s' if minutes != 1 else ''}, {seconds:.2f} seconds\"\n",
    "    if minutes > 0:\n",
    "        return f\"{minutes} minute{'s' if minutes != 1 else ''}, {seconds:.2f} seconds\"\n",
    "    return f\"{seconds:.2f} seconds\"\n",
    "\n",
    "def export_results(rewards, violations, save_name=None):\n",
    "    \"\"\"Function to export results to an Excel file.\"\"\"\n",
    "    # Create Pandas dataframe to hold results\n",
    "    to_save = pd.DataFrame({\"Rewards\": rewards, \"Violations\": violations})\n",
    "   \n",
    "    # Export the DataFrame to Excel.\n",
    "    if save_name:\n",
    "        file_name = save_name + '.xlsx'\n",
    "    else:\n",
    "        file_name = \"Results_\" + datetime.now().strftime('%d.%m.%Y_%H.%M') + '.xlsx'\n",
    "    writer = pd.ExcelWriter(file_name, engine='xlsxwriter')\n",
    "    to_save.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "    writer.close()\n",
    "    print(f\"Save sucessful: {file_name}. Episodes: {len(to_save)}\")\n",
    "\n",
    "def import_results(file_name):\n",
    "    \"\"\"Function to load results and violations from Excel and return as lists.\"\"\"\n",
    "    df = pd.read_excel(file_name)\n",
    "    print(f\"{file_name} imported. Episodes: {len(df)}\")    \n",
    "    return df['Rewards'].tolist(), df[\"Violations\"].tolist()\n",
    "\n",
    "def dec4(n):\n",
    "    \"\"\"Convert a number to a number with four decimal places.\"\"\"\n",
    "    return float(f\"{n:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5eff9",
   "metadata": {},
   "source": [
    "#### Agent Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4c8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTracker():\n",
    "    \"\"\"Class to store the actions the agent has made at each coordinate.\n",
    "    Can return the results in a coordinate grid for combination with original map.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.action_table = {}\n",
    "    \n",
    "    def add_action(self, coordinates, action):\n",
    "        # Function to add an action and the cosesponding coordinates if they are not already present.\n",
    "        if coordinates in self.action_table:\n",
    "            self.action_table[coordinates][action] += 1 # Increment action count\n",
    "        else:\n",
    "            self.action_table[coordinates] = [0, 0, 0 , 0] # Initiate list of four zero counts for actions.\n",
    "            self.action_table[coordinates][action] += 1 # Increment action count\n",
    "    \n",
    "    def action_grid(self):\n",
    "        grid = [[0 for j in range(15)] for i in range(15)] # Initiate empty grid\n",
    "        # 10, 11, 12, 13 represent the actions \"North\" \"South\", \"East\" and \"West\"\n",
    "        for i in self.action_table:\n",
    "            # For each coordinate, get the index of largest value add 10 and that becomes the grid value\n",
    "            grid[i[0]][i[1]] = self.action_table[i].index(max(self.action_table[i])) + 10\n",
    "   \n",
    "        return grid\n",
    "\n",
    "\n",
    "def visualize_maze(maze=INITIAL_MAZE, cell_size=30):\n",
    "    \"\"\"Visualize a Pac-Man maze using Tkinter. Written by Grok3\"\"\"\n",
    "    # Calculate window size based on maze dimensions\n",
    "    rows, cols = len(maze), len(maze[0])\n",
    "    window_size = rows * cell_size\n",
    "\n",
    "    # Set up Tkinter window\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Pac-Man Maze Visualization\")\n",
    "    canvas = Canvas(root, width=window_size, height=window_size, bg=\"black\")\n",
    "    canvas.pack()\n",
    "    \n",
    "    # Pacman starting position is [1][1]\n",
    "    maze[1][1] = 4\n",
    "    \n",
    "    # Draw the maze\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x1, y1 = col * cell_size, row * cell_size\n",
    "            x2, y2 = x1 + cell_size, y1 + cell_size\n",
    "            xc, yc = (x1 + x2) / 2, (y1 + y2) / 2  # Center point\n",
    "            if maze[row][col] == 2:  # Food\n",
    "                canvas.create_oval(x1 + 12, y1 + 12, x2 - 12, y2 - 12, fill=\"white\")\n",
    "            elif maze[row][col] == 4:  # Pacman Spawn\n",
    "                canvas.create_oval(x1 + 20, y1 + 20, x2 - 20, y2 - 20, fill=\"red\")\n",
    "            elif maze[row][col] == 6:  # Ghost Spawn\n",
    "                canvas.create_oval(x1 + 20, y1 + 20, x2 - 20, y2 - 20, fill=\"green\")\n",
    "            elif maze[row][col] == 1:  # Wall\n",
    "                # Draw double blue lines for walls\n",
    "                if row == 0 or maze[row-1][col] != 1:\n",
    "                    canvas.create_line(x1, y1, x2, y1, fill=\"blue\", width=4)\n",
    "                    canvas.create_line(x1, y1 + 2, x2, y1 + 2, fill=\"blue\", width=4)\n",
    "                if row == rows - 1 or maze[row+1][col] != 1:\n",
    "                    canvas.create_line(x1, y2, x2, y2, fill=\"blue\", width=4)\n",
    "                    canvas.create_line(x1, y2 - 2, x2, y2 - 2, fill=\"blue\", width=4)\n",
    "                if col == 0 or maze[row][col-1] != 1:\n",
    "                    canvas.create_line(x1, y1, x1, y2, fill=\"blue\", width=4)\n",
    "                    canvas.create_line(x1 + 2, y1, x1 + 2, y2, fill=\"blue\", width=4)\n",
    "                if col == cols - 1 or maze[row][col+1] != 1:\n",
    "                    canvas.create_line(x2, y1, x2, y2, fill=\"blue\", width=4)\n",
    "                    canvas.create_line(x2 - 2, y1, x2 - 2, y2, fill=\"blue\", width=4)\n",
    "            elif maze[row][col] == 5:  # Draw Firepit\n",
    "                # Red base flame (largest)\n",
    "                canvas.create_polygon(\n",
    "                    xc - 10, y2 - 5,  # Bottom left\n",
    "                    xc + 10, y2 - 5,  # Bottom right\n",
    "                    xc + 8, yc,       # Right mid\n",
    "                    xc + 2, y1 + 5,   # Top right\n",
    "                    xc - 2, y1 + 8,   # Top left\n",
    "                    xc - 8, yc,       # Left mid\n",
    "                    fill=\"red\", outline=\"red\"\n",
    "                )\n",
    "                # Orange middle flame\n",
    "                canvas.create_polygon(\n",
    "                    xc - 7, y2 - 3,   # Bottom left\n",
    "                    xc + 7, y2 - 3,   # Bottom right\n",
    "                    xc + 5, yc + 2,   # Right mid\n",
    "                    xc + 1, y1 + 8,   # Top right\n",
    "                    xc - 1, y1 + 10,  # Top left\n",
    "                    xc - 5, yc + 2,   # Left mid\n",
    "                    fill=\"orange\", outline=\"orange\"\n",
    "                )\n",
    "                # Yellow inner flame (smallest)\n",
    "                canvas.create_polygon(\n",
    "                    xc - 4, y2 - 1,   # Bottom left\n",
    "                    xc + 4, y2 - 1,   # Bottom right\n",
    "                    xc + 3, yc + 4,   # Right mid\n",
    "                    xc, y1 + 10,      # Top\n",
    "                    xc - 3, yc + 4,   # Left mid\n",
    "                    fill=\"yellow\", outline=\"yellow\"\n",
    "                )\n",
    "            elif maze[row][col] == 10:  # Up Arrow\n",
    "                canvas.create_polygon(\n",
    "                    xc, y1 + 10,          # Top point\n",
    "                    xc - 10, y1 + 20,     # Bottom left\n",
    "                    xc + 10, y1 + 20,     # Bottom right\n",
    "                    fill=\"cyan\", outline=\"magenta\"\n",
    "                )\n",
    "            elif maze[row][col] == 11:  # Down Arrow\n",
    "                canvas.create_polygon(\n",
    "                    xc, y2 - 10,          # Bottom point\n",
    "                    xc - 10, y2 - 20,     # Top left\n",
    "                    xc + 10, y2 - 20,     # Top right\n",
    "                    fill=\"cyan\", outline=\"magenta\"\n",
    "                )\n",
    "            elif maze[row][col] == 13:  # Left Arrow\n",
    "                canvas.create_polygon(\n",
    "                    x1 + 10, yc,          # Left point\n",
    "                    x1 + 20, yc - 10,     # Top right\n",
    "                    x1 + 20, yc + 10,     # Bottom right\n",
    "                    fill=\"cyan\", outline=\"magenta\"\n",
    "                )\n",
    "            elif maze[row][col] == 12:  # Right Arrow\n",
    "                canvas.create_polygon(\n",
    "                    x2 - 10, yc,          # Right point\n",
    "                    x2 - 20, yc - 10,     # Top left\n",
    "                    x2 - 20, yc + 10,     # Bottom left\n",
    "                    fill=\"cyan\", outline=\"magenta\"\n",
    "                )\n",
    "\n",
    "\n",
    "    # Start the Tkinter event loop\n",
    "    root.mainloop()\n",
    "\n",
    "def maze_and_route(maze, actions):\n",
    "    \"\"\"Function to combine a maze with a route for visualisation purposes.\"\"\"\n",
    "    maze = copy.deepcopy(maze)\n",
    "    for i in range(len(maze)):\n",
    "        for j in range(len(maze[0])):\n",
    "            if maze[i][j] not in [2, 5]:\n",
    "                maze[i][j] = maze[i][j] + actions[i][j]\n",
    "                \n",
    "    return maze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38884a3",
   "metadata": {},
   "source": [
    "#### Neural Bayes Shielding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesShield():\n",
    "    \"\"\"Shield class\"\"\"\n",
    "    def __init__(self, mult=1, threshold=None):\n",
    "        self.classifier = GaussianNB() # Initiate classifier\n",
    "        self.train_count = 0\n",
    "        self.mult = mult\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    \n",
    "    def balance_samples(self, df, mult=1):    \n",
    "        \"\"\"Function to take in a dataframe of samples and balance the data by undersampling the safe states.\n",
    "           Baseline make it a 1:1 Ratio.\n",
    "           The mult paramater changes this. 2 means 2:1 - Safe:Unsafe\n",
    "           Mult of 0 returns the original data\"\"\"\n",
    "        # What it does: Randomly reduce the number of Class A samples to match Class B (e.g., 1,000 each). [Notes to delete]\n",
    "        # Pros: Simple, reduces training time, balances classes perfectly.\n",
    "        # Cons: Throws away a lot of data (26,000 Class A samples), potentially losing valuable patterns.\n",
    "        if mult == 0:\n",
    "            # Original data returned.\n",
    "            return df\n",
    "\n",
    "        unsafe_exps = df[df['Safe'] == 0]\n",
    "        safe_exps = df[df['Safe'] == 1]\n",
    "\n",
    "        # Undersample the safe experiences with no duplicates to match the number of unsafe ones present.\n",
    "        undersampled_safe_exps = resample(safe_exps,replace=False, n_samples=int(len(unsafe_exps)*mult), random_state=6)\n",
    "\n",
    "        # Combine the unsafe with undersampled safe.\n",
    "        return pd.concat([unsafe_exps, undersampled_safe_exps])\n",
    "\n",
    "    def train(self, data):\n",
    "        # Function to train the classifier with new data.\n",
    "        # Add column names to the data.\n",
    "        data.columns = [\"episode_number\", \"Pac_x\", \"Pac_y\", \"G1_x\", \"G1_y\", \"G2_x\", \"G2_y\", \"F_1\", \"F_2\", \"Action\", \"Safe\"]\n",
    "        \n",
    "        # Balance data with input multiplier.\n",
    "        balanced_data = self.balance_samples(data, self.mult)\n",
    "          \n",
    "        X_train = balanced_data.drop(balanced_data.columns[[0, 10]], axis=1) # Drop columns 0 and 10 (episode and safe)\n",
    "        Y_train = balanced_data[\"Safe\"]\n",
    "        \n",
    "        # Train Classifier with balanced data.\n",
    "        self.classifier.partial_fit(X_train, Y_train, classes=[0, 1])\n",
    "        \n",
    "        self.train_count += 1 # Increment training count.\n",
    "    \n",
    "    def predict(self, exp):\n",
    "        # Fuction to make a prediction on a data point.\n",
    "        if self.threshold:\n",
    "            # Use probability threshold (1 for safe class, 0 for unsafe class).\n",
    "            Y_probs = self.classifier.predict_proba([exp])\n",
    "            return (Y_probs[:, 1] >= self.threshold).astype(int)[0]\n",
    "        else:\n",
    "            return self.classifier.predict([exp])[0]\n",
    "    \n",
    "    def train_count(self):\n",
    "        return self.train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3486fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShieldMemory():\n",
    "    \"\"\"Class to store experiences:\n",
    "    Format is (current_state, action, terminated)\"\"\"\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "    \n",
    "    def add_experience(self, exp):\n",
    "        # Adds an experience to replay buffer.\n",
    "        # Input format for exp: (state, action, safe, episode_number)\n",
    "        # Experience format is a list of all of these combined. [episode_number, state, action, safe]\n",
    "        experience = [exp[3]]\n",
    "        for n in exp[0]:\n",
    "            # Round to four decimal places\n",
    "            experience.append(n) # dec4() not needed\n",
    "        # experience += exp[0] \n",
    "        experience.append(exp[1])\n",
    "        experience.append(exp[2])\n",
    "        \n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def current_memories(self):\n",
    "        # Returns how many memories are currently stored.\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def export_memories(self):\n",
    "        \"\"\"Function to export memories to an Excel file.\"\"\"\n",
    "        # Create Pandas datafram to hold results\n",
    "        df = pd.DataFrame(self.memory)\n",
    "        # df.round(5) # Round to 5 decimal places to save storage/compute?\n",
    "        df.columns = [\"Ep_#\", \"Pac_x\", \"Pac_y\", \"G1_x\", \"G1_y\", \"G2_x\", \"G2_y\", \"F_1\", \"F_2\", \"Actn\", \"Safe\"]\n",
    "\n",
    "        # Export the DataFrame to Excel.\n",
    "        df.to_csv(\"Shield_Experience_\" + datetime.now().strftime('%d.%m.%Y_%H.%M')+\".csv\")\n",
    "        print(\"Save sucessful\")\n",
    "        \n",
    "    def wipe_memory(self):\n",
    "        # Wipe memory.\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a52209c",
   "metadata": {},
   "source": [
    "### Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e760e80-5144-4f43-9089-d326bffaa163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    \"\"\"Class to store experiences for training purposes.\n",
    "       Experiences are store in the format (current_state, action, new_state, reward, terminated)\"\"\"\n",
    "    def __init__(self, memory_max=500000, batch_size=32):\n",
    "        self.memory = deque([], maxlen=memory_max) # Maximum experiences stored\n",
    "        self.batch_size = batch_size # Batch size\n",
    "    \n",
    "    def add_experience(self, exp):\n",
    "        # Adds an experience to replay buffer.\n",
    "        # Format (current_state, action, new_state, reward, terminated)\n",
    "        self.memory.append(exp)\n",
    "    \n",
    "    def sample(self):\n",
    "        # Returns a batch of samples from memory.\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "    def current_memories(self):\n",
    "        # Returns how many memories are currently stored.\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Class for the network model.\"\"\"\n",
    "    def __init__(self, in_states, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, 32) \n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.out = nn.Linear(16, out_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network.\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class CoordinateMemory():\n",
    "    # Class to store coordinates explored for exploration bonus purposes.\n",
    "    def __init__(self):\n",
    "        # Ititialise a set with Pacmans starting coordinates.\n",
    "        self.memory = set((1, 1))\n",
    "    \n",
    "    def add(self, pos):\n",
    "        # Adds an explored coordinate to memory and returns if it is newly explored.\n",
    "        pos = tuple(pos)\n",
    "        new = True # Is the coordinate newly explored, or not.\n",
    "        if pos in self.memory:\n",
    "            new = False\n",
    "        self.memory.add(pos)\n",
    "        \n",
    "        return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directions for movement, including Stop\n",
    "directions = {\n",
    "    \"North\": [-1, 0],\n",
    "    \"South\": [1, 0],\n",
    "    \"East\": [0, 1],\n",
    "    \"West\": [0, -1],\n",
    "    \"Stop\": [0, 0]\n",
    "}\n",
    "\n",
    "# Relative directions for right-hand and left-hand rules\n",
    "RIGHT_TURN = {\n",
    "    \"North\": \"East\",\n",
    "    \"East\": \"South\",\n",
    "    \"South\": \"West\",\n",
    "    \"West\": \"North\"\n",
    "}\n",
    "\n",
    "LEFT_TURN = {\n",
    "    \"North\": \"West\",\n",
    "    \"West\": \"South\",\n",
    "    \"South\": \"East\",\n",
    "    \"East\": \"North\"\n",
    "}\n",
    "\n",
    "OPPOSITE = {\n",
    "    \"North\": \"South\",\n",
    "    \"South\": \"North\",\n",
    "    \"East\": \"West\",\n",
    "    \"West\": \"East\"\n",
    "}\n",
    "\n",
    "# Game Settings\n",
    "CELL_SIZE = 30\n",
    "WINDOW_SIZE = len(INITIAL_MAZE) * CELL_SIZE\n",
    "WALL_THICKNESS = 4\n",
    "WALL_GAP = 2\n",
    "\n",
    "# Ghost movement probability\n",
    "GHOST_MOVE_PROBABILITY = 0.9\n",
    "\n",
    "\n",
    "class WallFollowingGhost:\n",
    "    \"\"\"Class for the wall following ghosts movement policy. Largely generated by Grok 3\n",
    "       Can be set to follow the left or right wall.\n",
    "       \n",
    "       Input: Colour, starting position, starting direction and follow side.\"\"\"\n",
    "    def __init__(self, color, start_pos, start_direction, follow_side=\"right\"):\n",
    "        self.color = color\n",
    "        self.current_pos = start_pos\n",
    "        self.direction = start_direction\n",
    "        self.follow_side = follow_side\n",
    "\n",
    "    def move(self, maze):\n",
    "        row, col = self.current_pos\n",
    "        if self.follow_side == \"right\":\n",
    "            # Follow side is left.\n",
    "            turn_dir = RIGHT_TURN[self.direction]\n",
    "            straight_dir = self.direction\n",
    "            left_dir = LEFT_TURN[self.direction]\n",
    "            back_dir = OPPOSITE[self.direction]\n",
    "            directions_to_try = [turn_dir, straight_dir, left_dir, back_dir]\n",
    "        else:\n",
    "            # Follow side is left\n",
    "            turn_dir = LEFT_TURN[self.direction]\n",
    "            straight_dir = self.direction\n",
    "            right_dir = RIGHT_TURN[self.direction]\n",
    "            back_dir = OPPOSITE[self.direction]\n",
    "            directions_to_try = [turn_dir, straight_dir, right_dir, back_dir]\n",
    "\n",
    "        for direction in directions_to_try:\n",
    "            delta_row, delta_col = directions[direction]\n",
    "            new_row = row + delta_row\n",
    "            new_col = col + delta_col\n",
    "            if (0 <= new_row < len(maze) and 0 <= new_col < len(maze[0]) and\n",
    "                    maze[new_row][new_col] != 1):\n",
    "                self.current_pos = [new_row, new_col]\n",
    "                self.direction = direction\n",
    "                break\n",
    "\n",
    "\n",
    "class PacmanGame:\n",
    "    \"\"\"Class to handle and draw the Pacman game enviroment. Largely generated by Grok 3.\"\"\"\n",
    "    def __init__(self, root, num_ghosts=1, visualize=True):\n",
    "        self.visualize = visualize\n",
    "        self.root = root if visualize else None\n",
    "        if visualize:\n",
    "            self.canvas = Canvas(self.root, width=WINDOW_SIZE, height=WINDOW_SIZE, bg=\"black\")\n",
    "            self.canvas.pack()\n",
    "        self.num_ghosts = max(1, min(4, num_ghosts))\n",
    "        self.ghost_colors = [\"red\", \"pink\", \"orange\", \"cyan\"][:self.num_ghosts]\n",
    "        self.ghost_start_positions = [[3, 5], [3, 10], [11, 4], [11, 10]][:self.num_ghosts] # Also set in self.ghosts\n",
    "        self.ghosts = [\n",
    "            # Red ghost (Blinky)\n",
    "            WallFollowingGhost(\"red\", [3, 5], \"South\", follow_side=\"right\"),\n",
    "            # Pink ghost (Pinky)\n",
    "            WallFollowingGhost(\"pink\", [3, 10], \"South\", follow_side=\"left\"),\n",
    "            # Orange ghost (Clyde)\n",
    "            WallFollowingGhost(\"orange\", [11, 4], \"North\", follow_side=\"right\"),\n",
    "            # Cyan ghost (Inky)\n",
    "            WallFollowingGhost(\"cyan\", [11, 10], \"North\", follow_side=\"left\")\n",
    "        ][:self.num_ghosts]\n",
    "        self.ghost_positions = [ghost.current_pos for ghost in self.ghosts]\n",
    "        self.ghost_modes = [\"wall_following\"] * self.num_ghosts\n",
    "        self.state_size = 2 + 2 * self.num_ghosts\n",
    "        self.action_size = 4\n",
    "        self.actions = [\"North\", \"South\", \"East\", \"West\"] # Available actions\n",
    "        self.reset_episode()\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.maze = [row[:] for row in INITIAL_MAZE]\n",
    "        self.pacman_pos = [1, 1]  \n",
    "        for i, ghost in enumerate(self.ghosts):\n",
    "            ghost.current_pos = self.ghost_start_positions[i]\n",
    "            if ghost.color == \"red\":\n",
    "                ghost.current_pos = [3, 4]\n",
    "                ghost.direction = \"South\"\n",
    "            elif ghost.color == \"pink\":\n",
    "                ghost.current_pos = [3, 10]\n",
    "                ghost.direction = \"South\"\n",
    "            elif ghost.color == \"orange\":\n",
    "                ghost.current_pos = [11, 4]\n",
    "                ghost.direction = \"North\"\n",
    "            elif ghost.color == \"cyan\":\n",
    "                ghost.current_pos = [11, 10]\n",
    "                ghost.direction = \"North\"\n",
    "        self.ghost_positions = [ghost.current_pos for ghost in self.ghosts]\n",
    "        self.food_left = sum(row.count(2) for row in self.maze)  # Count only initial 2s\n",
    "        self.running = True\n",
    "        self.pacman_direction = \"East\"\n",
    "        self.ghost_modes = [\"wall_following\"] * self.num_ghosts\n",
    "        if self.visualize:\n",
    "            self.draw_maze()\n",
    "            self.draw_pacman()\n",
    "            self.draw_ghosts()\n",
    "\n",
    "    def draw_maze(self):\n",
    "        if not self.visualize:\n",
    "            return\n",
    "        self.canvas.delete(\"all\")\n",
    "        rows, cols = len(self.maze), len(self.maze[0])\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                x1, y1 = col * CELL_SIZE, row * CELL_SIZE\n",
    "                x2, y2 = x1 + CELL_SIZE, y1 + CELL_SIZE\n",
    "                if self.maze[row][col] == 2:\n",
    "                    self.canvas.create_oval(x1 + 12, y1 + 12, x2 - 12, y2 - 12, fill=\"white\")\n",
    "                if self.maze[row][col] == 1:\n",
    "                    if row == 0 or self.maze[row-1][col] != 1:\n",
    "                        self.canvas.create_line(x1, y1, x2, y1, fill=\"blue\", width=WALL_THICKNESS)\n",
    "                        self.canvas.create_line(x1, y1 + WALL_GAP, x2, y1 + WALL_GAP, fill=\"blue\", width=WALL_THICKNESS)\n",
    "                    if row == rows - 1 or self.maze[row+1][col] != 1:\n",
    "                        self.canvas.create_line(x1, y2, x2, y2, fill=\"blue\", width=WALL_THICKNESS)\n",
    "                        self.canvas.create_line(x1, y2 - WALL_GAP, x2, y2 - WALL_GAP, fill=\"blue\", width=WALL_THICKNESS)\n",
    "                    if col == 0 or self.maze[row][col-1] != 1:\n",
    "                        self.canvas.create_line(x1, y1, x1, y2, fill=\"blue\", width=WALL_THICKNESS)\n",
    "                        self.canvas.create_line(x1 + WALL_GAP, y1, x1 + WALL_GAP, y2, fill=\"blue\", width=WALL_THICKNESS)\n",
    "                    if col == cols - 1 or self.maze[row][col+1] != 1:\n",
    "                        self.canvas.create_line(x2, y1, x2, y2, fill=\"blue\", width=WALL_THICKNESS)\n",
    "                        self.canvas.create_line(x2 - WALL_GAP, y1, x2 - WALL_GAP, y2, fill=\"blue\", width=WALL_THICKNESS)\n",
    "                if self.maze[row][col] == 5:  # Draw Firepit\n",
    "                    xc, yc = (x1 + x2) / 2, (y1 + y2) / 2  # Center point\n",
    "                    # Red base flame (largest)\n",
    "                    self.canvas.create_polygon(\n",
    "                        xc - 10, y2 - 5,  # Bottom left\n",
    "                        xc + 10, y2 - 5,  # Bottom right\n",
    "                        xc + 8, yc,       # Right mid\n",
    "                        xc + 2, y1 + 5,   # Top right\n",
    "                        xc - 2, y1 + 8,   # Top left\n",
    "                        xc - 8, yc,       # Left mid\n",
    "                        fill=\"red\", outline=\"red\"\n",
    "                    )\n",
    "                    # Orange middle flame\n",
    "                    self.canvas.create_polygon(\n",
    "                        xc - 7, y2 - 3,   # Bottom left\n",
    "                        xc + 7, y2 - 3,   # Bottom right\n",
    "                        xc + 5, yc + 2,   # Right mid\n",
    "                        xc + 1, y1 + 8,   # Top right\n",
    "                        xc - 1, y1 + 10,  # Top left\n",
    "                        xc - 5, yc + 2,   # Left mid\n",
    "                        fill=\"orange\", outline=\"orange\"\n",
    "                    )\n",
    "                    # Yellow inner flame (smallest)\n",
    "                    self.canvas.create_polygon(\n",
    "                        xc - 4, y2 - 1,   # Bottom left\n",
    "                        xc + 4, y2 - 1,   # Bottom right\n",
    "                        xc + 3, yc + 4,   # Right mid\n",
    "                        xc, y1 + 10,      # Top\n",
    "                        xc - 3, yc + 4,   # Left mid\n",
    "                        fill=\"yellow\", outline=\"yellow\"\n",
    "                    )\n",
    "\n",
    "\n",
    "    def draw_pacman(self):\n",
    "        \"\"\"Function to draw the Pacman in the Pacman game. Generated by Grok 3\"\"\"\n",
    "        if not self.visualize:\n",
    "            return\n",
    "        row, col = self.pacman_pos\n",
    "        x, y = col * CELL_SIZE + CELL_SIZE // 2, row * CELL_SIZE + CELL_SIZE // 2\n",
    "        radius = CELL_SIZE // 2 - 2\n",
    "        if self.pacman_direction == \"East\":\n",
    "            start, extent = 30, 300\n",
    "        elif self.pacman_direction == \"West\":\n",
    "            start, extent = 210, 300\n",
    "        elif self.pacman_direction == \"North\":\n",
    "            start, extent = 120, 300\n",
    "        else:\n",
    "            start, extent = 300, 300\n",
    "        self.pacman = self.canvas.create_arc(\n",
    "            x - radius, y - radius, x + radius, y + radius,\n",
    "            start=start, extent=extent, fill=\"yellow\", outline=\"yellow\"\n",
    "        )\n",
    "\n",
    "    def draw_ghosts(self):\n",
    "        \"\"\"Function to draw the ghots in the Pacman game. Generated by Grok 3\"\"\"\n",
    "        if not self.visualize:\n",
    "            return\n",
    "        self.ghosts_drawn = []\n",
    "        for i, ghost in enumerate(self.ghosts):\n",
    "            row, col = ghost.current_pos\n",
    "            x, y = col * CELL_SIZE + CELL_SIZE // 2, row * CELL_SIZE + CELL_SIZE // 2\n",
    "            radius = CELL_SIZE // 2 - 2\n",
    "            points = []\n",
    "            num_points = 10\n",
    "            for j in range(num_points + 1):\n",
    "                angle = np.pi * j / num_points\n",
    "                px = x - radius * np.cos(angle)\n",
    "                py = y - radius * np.sin(angle)\n",
    "                points.append(px)\n",
    "                points.append(py)\n",
    "            points.extend([\n",
    "                x + radius, y + radius - 5,\n",
    "                x + radius * 0.75, y + radius,\n",
    "                x + radius * 0.25, y + radius - 5,\n",
    "                x - radius * 0.25, y + radius,\n",
    "                x - radius * 0.75, y + radius - 5,\n",
    "                x - radius, y + radius - 5,\n",
    "                x - radius, y\n",
    "            ])\n",
    "            body = self.canvas.create_polygon(points, fill=ghost.color, outline=ghost.color, smooth=True)\n",
    "            eye_radius = radius // 3\n",
    "            pupil_radius = eye_radius // 2\n",
    "            eye_y_offset = y - radius * 0.5\n",
    "            self.canvas.create_oval(x - radius * 0.5 - eye_radius, eye_y_offset - eye_radius,\n",
    "                                    x - radius * 0.5 + eye_radius, eye_y_offset + eye_radius,\n",
    "                                    fill=\"white\")\n",
    "            self.canvas.create_oval(x + radius * 0.5 - eye_radius, eye_y_offset - eye_radius,\n",
    "                                    x + radius * 0.5 + eye_radius, eye_y_offset + eye_radius,\n",
    "                                    fill=\"white\")\n",
    "            self.canvas.create_oval(x - radius * 0.5 - pupil_radius, eye_y_offset - pupil_radius,\n",
    "                                    x - radius * 0.5 + pupil_radius, eye_y_offset + pupil_radius,\n",
    "                                    fill=\"black\")\n",
    "            self.canvas.create_oval(x + radius * 0.5 - pupil_radius, eye_y_offset - pupil_radius,\n",
    "                                    x + radius * 0.5 + pupil_radius, eye_y_offset + pupil_radius,\n",
    "                                    fill=\"black\")\n",
    "            self.ghosts_drawn.append(body)\n",
    "\n",
    "    def get_legal_directions(self, pos):\n",
    "        legal_directions = []\n",
    "        for direction, (delta_row, delta_col) in directions.items():\n",
    "            if direction == \"Stop\":\n",
    "                continue\n",
    "            new_row = pos[0] + delta_row\n",
    "            new_col = pos[1] + delta_col\n",
    "            if 0 <= new_row < len(self.maze) and 0 <= new_col < len(self.maze[0]) and self.maze[new_row][new_col] != 1:\n",
    "                legal_directions.append(direction)\n",
    "        return legal_directions\n",
    "\n",
    "    def move_ghost(self, ghost_idx, ghost_pos):\n",
    "        if random.random() > GHOST_MOVE_PROBABILITY:\n",
    "            return\n",
    "        if ghost_idx >= self.num_ghosts:\n",
    "            return\n",
    "\n",
    "        self.ghosts[ghost_idx].move(self.maze)\n",
    "        self.ghost_positions[ghost_idx] = self.ghosts[ghost_idx].current_pos\n",
    "\n",
    "    def move_pacman(self, action):\n",
    "        \"\"\"Function to control the agents movemnets on the grid when an action is given.\"\"\"\n",
    "        delta_row, delta_col = directions[action]\n",
    "        new_row = self.pacman_pos[0] + delta_row\n",
    "        new_col = self.pacman_pos[1] + delta_col\n",
    "        reward = 0\n",
    "        if action != \"Stop\":\n",
    "            self.pacman_direction = action\n",
    "        if 0 <= new_row < len(self.maze) and 0 <= new_col < len(self.maze[0]) and self.maze[new_row][new_col] != 1:\n",
    "            self.pacman_pos[0] = new_row\n",
    "            self.pacman_pos[1] = new_col\n",
    "            if self.maze[new_row][new_col] == 2:\n",
    "                self.maze[new_row][new_col] = 0\n",
    "                self.food_left -= 1\n",
    "                reward = 0.8 # Reward for collecting food\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def get_state(self, tensor=True):\n",
    "        \"\"\"Function to return a Tensor of the state of the environment.\n",
    "           This is in the format of pacmans coordinates, then each ghosts coordinates,\n",
    "           then if two pieces of food are present.  Written by PBN\"\"\"\n",
    "        observations = [] # The state to be returned\n",
    "        \n",
    "        # Firepit Food                Simple Map Food\n",
    "        #print(INITIAL_MAZE[6][11])   self.maze[3][7]\n",
    "        # print(INITIAL_MAZE[7][1])   self.maze[1][12]\n",
    "        \n",
    "        # Add Pacmans's x and y coordinates.\n",
    "        observations.append(self.pacman_pos[1]/14) # Pacmans's X position\n",
    "        observations.append(self.pacman_pos[0]/6) # Pacmans's Y position\n",
    "        # Add each Ghost's x and y coordinates. Normalised (with /14 and /6).\n",
    "        for i, ghost_pos in enumerate(self.ghost_positions):\n",
    "            observations.append(ghost_pos[1]/14)\n",
    "            observations.append(ghost_pos[0]/6)\n",
    "        # Add if each food is still present.\n",
    "        if self.maze[6][11] == 2:\n",
    "            observations.append(1) # Piece of food 1\n",
    "        else:\n",
    "            observations.append(0)\n",
    "        if self.maze[7][1] == 2:\n",
    "            observations.append(1) # Piece of food 2\n",
    "        else:\n",
    "            observations.append(0)\n",
    "\n",
    "        if tensor:\n",
    "            return torch.FloatTensor(observations)\n",
    "        else:\n",
    "            return observations\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Function to perform one step in the environment with the givven action.\n",
    "           The position of the agent (Pacman) and the hazards (Ghosts) is updated.\n",
    "           The status of any eaten food is also updated.\"\"\"\n",
    "        if self.running:\n",
    "            reward = self.move_pacman(action) # Reward if food is eaten.\n",
    "            for idx, ghost_pos in enumerate(self.ghost_positions):\n",
    "                self.move_ghost(idx, ghost_pos)\n",
    "\n",
    "            new_state = self.get_state()\n",
    "            terminated = False\n",
    "            if any(self.pacman_pos == ghost_pos for ghost_pos in self.ghost_positions):\n",
    "                # Check if Pacman is in the same position as any ghosts.\n",
    "                self.running = False\n",
    "                terminated = True\n",
    "            elif self.food_left == 0:\n",
    "                # If food left is zero, Pacman has won the game.\n",
    "                self.running = False\n",
    "                terminated = True\n",
    "            elif self.maze[self.pacman_pos[0]][self.pacman_pos[1]] == 5:  # Check for firepit\n",
    "                # print(\"Hit Firepit at \", self.pacman_pos)\n",
    "                self.running = False\n",
    "                terminated = True\n",
    "\n",
    "            if self.visualize:\n",
    "                self.draw_maze()\n",
    "                self.draw_pacman()\n",
    "                self.draw_ghosts()\n",
    "\n",
    "        return new_state, reward, terminated\n",
    "\n",
    "\n",
    "def run_episodes(num_episodes, num_ghosts=1, visualize=True, move_limit=100, random_actions=False, DDQN=False, nb_shield=False, mult=1, threshold=None):\n",
    "    # Training using the Pacman environment by Grok 3 with DQN/DDQN network.\n",
    "    total_start_time = time.time()\n",
    "    epsilon = 1 # E-greedy starting policy, starting at 100% randomised movement.\n",
    "    decay = 0.0001 # Epsilon decay per episode [0.0001=10,000 episodes]\n",
    "    root = tk.Tk() if visualize else None\n",
    "    if visualize:\n",
    "        root.title(f\"Pac-Man with {num_ghosts} Ghost(s)\")\n",
    "    game = PacmanGame(root, num_ghosts, visualize)\n",
    "    actions = [\"North\", \"South\", \"East\", \"West\"]\n",
    "    \n",
    "    # Statistics\n",
    "    violations = 0 # Safety violations.\n",
    "    timeouts = 0 # Amount of time the move limit is hit.\n",
    "    reward_per_episode = []\n",
    "    violations_per_episode = []\n",
    "    \n",
    "    # DQN\n",
    "    loss_function = nn.MSELoss()\n",
    "    memory = ExperienceReplay()\n",
    "    discount_factor = 0.9 # The % of the reward that is propagated back to the the previous state.\n",
    "    learning_rate = 0.001 # Learning rate for DQN.\n",
    "    network_sync_rate = 100 # The rate at which the policy and target networks syncronise.\n",
    "    input_len = 8\n",
    "    num_actions = 4\n",
    "    memory = ExperienceReplay() # Initialise Experience Replay.\n",
    "    step_count = 0 # Steps between each target network update.\n",
    "\n",
    "    # Create policy and target network\n",
    "    policy_dqn = DQN(in_states=input_len, out_actions=num_actions)\n",
    "    target_dqn = DQN(in_states=input_len, out_actions=num_actions)\n",
    "    target_dqn.load_state_dict(policy_dqn.state_dict()) # Copy target networks weights from policy network.\n",
    "    optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=learning_rate) # Set Adam optimizer.\n",
    "    \n",
    "    # Shielding Functionality\n",
    "    shield_memory = ShieldMemory() # Initialise Shield memory storage.\n",
    "    shield = NaiveBayesShield(mult, threshold) # Initialise Shield\n",
    "    \n",
    "    # Initialise Route Tracker\n",
    "    tracker = AgentTracker()\n",
    "    \n",
    "    def optimize(mini_batch, policy_dqn, target_dqn):\n",
    "        # Function to perform one training step.\n",
    "        current_q_list = [] # List of current calculated q values.\n",
    "        target_q_list = [] # List of target q values.\n",
    "\n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    # No exploration reward on death.\n",
    "                    reward = 0\n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                # Calculate Q value using Double DQN Algorithm.\n",
    "                # Get best action from the policy network.\n",
    "                if DDQN:\n",
    "                    best_action = policy_dqn(state).argmax()\n",
    "                    with torch.no_grad():\n",
    "                        target = torch.FloatTensor(reward + discount_factor * target_dqn(new_state)[best_action])\n",
    "                else: \n",
    "                    # Calculate Q value for DQN.\n",
    "                    with torch.no_grad():\n",
    "                        target = torch.FloatTensor(reward + discount_factor * target_dqn(new_state).max())\n",
    "        \n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(state)\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(state)\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "        \n",
    "        # Compute loss for the whole minibatch\n",
    "        loss = loss_function(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "        \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # Train for required number of episodes.\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        # Exploration Bonus Coordinate Storage\n",
    "        exp_memory = CoordinateMemory()\n",
    "        \n",
    "        if (episode) % 50000 == 0:\n",
    "            this_time = time.time()\n",
    "            print(f\"Time {format_time(time.time() - total_start_time)} Starting Episode {episode}...\")\n",
    "            \n",
    "        game.reset_episode() # Reset environment to initial state.  \n",
    "        state = game.get_state() # Get initial game state.\n",
    "        while game.running:\n",
    "            random_action = False\n",
    "            if random.uniform(0, 1) > epsilon and not random_actions:\n",
    "                # Select best action\n",
    "                with torch.no_grad():\n",
    "                    action_ind = policy_dqn(state).argmax().item()\n",
    "                    if nb_shield:\n",
    "                        # Get list of Actions sorted in descending Q-value order.\n",
    "                        q_vals = policy_dqn(state).tolist() # Get Q-values for current state.\n",
    "                        ind_q_vals = list(enumerate(q_vals)) # Enumerate to get index of each value.\n",
    "                        sorted_indices = [ind for ind, val in sorted(ind_q_vals, key=lambda x: x[1], reverse=True)]\n",
    "                    action = actions[action_ind]\n",
    "            else:\n",
    "                # Random action\n",
    "                action = random.sample(actions, 1)[0]\n",
    "                random_action = True\n",
    "            \n",
    "            \"\"\"Shield Functionality\"\"\"\n",
    "            if nb_shield:       \n",
    "                # Get state for shield in list format.\n",
    "                shield_state = game.get_state(False) \n",
    "            \n",
    "            if nb_shield and shield.train_count > 0 and not random_action:\n",
    "                # Only activate shield if it has been trained at least once and is not a random action.\n",
    "                if not random_action:\n",
    "                    for action_ind in sorted_indices:\n",
    "                        # If no action deemed safe, original action will be taken\n",
    "                        pred_state = shield_state.copy()\n",
    "                        pred_state.append(action_ind)\n",
    "\n",
    "                        # Make prediction of the safety of the action.\n",
    "                        safe = shield.predict(pred_state)\n",
    "                        if safe:\n",
    "                            # If action is deemed safe, break loop, that action will be taken.\n",
    "                            action = actions[action_ind]\n",
    "                            break\n",
    "\n",
    "\n",
    "            # Take action\n",
    "            new_state, reward, terminated = game.step(action)\n",
    "            \n",
    "            new_exploration = exp_memory.add(game.pacman_pos)\n",
    "            if new_exploration:\n",
    "                # Extra reward is new coordinates explored.\n",
    "                reward += 0.1\n",
    "\n",
    "            total_reward += reward\n",
    "            \n",
    "            steps += 1 # Increment step counter\n",
    "            step_count += 1 # Increment target network step counter\n",
    "            \n",
    "            if steps > move_limit:\n",
    "                # Truncate episode if move limit is hit.\n",
    "                game.running = False\n",
    "            \n",
    "            # Save experience to memory\n",
    "            # Convert action back to it's relevant index.\n",
    "            action_ind = actions.index(action)\n",
    "            #Format (current_state, action, new_state, reward, terminated)\n",
    "            memory.add_experience((state, action_ind, new_state, reward, terminated))\n",
    "            \n",
    "            # Add to Coordiantes and action to agent tracker\n",
    "            tracker.add_action(tuple(game.pacman_pos), action_ind)\n",
    "            \n",
    "            if nb_shield:\n",
    "                # Save experience to shield memory\n",
    "                if terminated and reward < 0.2:\n",
    "                    # If terminated and a low reward, must have hit a ghost\n",
    "                    safe = 0\n",
    "                else:\n",
    "                    # \n",
    "                    safe = 1\n",
    "                shield_memory.add_experience((shield_state, action_ind, safe, episode))\n",
    "\n",
    "            # New state becomes current state.\n",
    "            state = new_state\n",
    "            \n",
    "            if episode > 1000:\n",
    "                # Decrease epsilon after each episode to min of 0.05.\n",
    "                epsilon = max(epsilon - decay, 0.05)\n",
    "            \n",
    "            if visualize and root:\n",
    "                root.update()\n",
    "                root.after(100)\n",
    "\n",
    "        \n",
    "        # Check the outcome of the episode.\n",
    "        if any(game.pacman_pos == ghost_pos for ghost_pos in game.ghost_positions):\n",
    "            outcome = \"Lost (caught by ghost)\"\n",
    "        elif game.food_left == 0:\n",
    "            outcome = \"Won (ate all food)\"\n",
    "        else:\n",
    "            outcome = f\"Exceeded {move_limit} moves\"\n",
    "    \n",
    "        # Track reward and safety violations.\n",
    "        reward_per_episode.append(total_reward)\n",
    "        if outcome == \"Lost (caught by ghost)\":\n",
    "            violations += 1 # Track violations.\n",
    "            violations_per_episode.append(1)\n",
    "        else:\n",
    "            violations_per_episode.append(0)\n",
    "        \n",
    "        if outcome == f\"Exceeded {move_limit} moves\":\n",
    "            timeouts += 1 # Track move limit violations.\n",
    "            \n",
    "        if not random_actions:\n",
    "            for _ in range(5):\n",
    "                # Five optimisations per episode.\n",
    "                # Check if enough experience has been collected.\n",
    "                if memory.current_memories() > 32:\n",
    "                    mini_batch = memory.sample() # Collect a sample of data.\n",
    "                    optimize(mini_batch, policy_dqn, target_dqn) # Train networks with sample.\n",
    "\n",
    "                    # Update target network weights to policy network weights every x steps.\n",
    "                    if step_count > network_sync_rate:\n",
    "                        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                        step_count = 0 # Reset counter\n",
    "                    \n",
    "        if episode % 10000 == 0 and nb_shield:\n",
    "            #print(f\"Training shield at episode {episode}, current samples {shield_memory.current_memories()}, {episode - violations - timeouts} episodes won.\")\n",
    "            # Convert current shield memory to a dataframe\n",
    "            training_data = pd.DataFrame(shield_memory.memory)\n",
    "            \n",
    "            # Train shield with the data and wipe the memory as data has been used.\n",
    "            shield.train(training_data)\n",
    "            shield_memory.wipe_memory()\n",
    "            #print(f\"Training complete - Current samples {shield_memory.current_memories()}\")\n",
    "            \n",
    "         \n",
    "    # print(\"Shield memories:\", shield_memory.current_memories())\n",
    "    # print(\"Exporting shield data\")\n",
    "    # shield_memory.export_memories() ------------------------------------------------------------ MEMORY EXPORT FOR SHIELD\n",
    "    \n",
    "    # Create action grid for output\n",
    "    route = tracker.action_grid()  \n",
    "    \n",
    "    total_end_time = time.time()\n",
    "    total_duration = total_end_time - total_start_time\n",
    "    #print(f\"\\nRuntime for {num_episodes} episodes: {format_time(total_duration)}\")\n",
    "    #print(f\"{format_time(total_duration/num_episodes)} per episode\")\n",
    "    print(f\"{num_episodes - violations - timeouts} episodes won.\")\n",
    "    print(f\"Total safety violations: {violations}\")\n",
    "    print(f\"Move limit hit: {timeouts}\")\n",
    "    print()\n",
    "    if visualize and root:\n",
    "        root.destroy()\n",
    "    \n",
    "    return reward_per_episode, violations, violations_per_episode, route"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881eab5-0959-4b0e-80f2-66223c81e61e",
   "metadata": {},
   "source": [
    "#### DQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "NUM_EPISODES = 200000 # Number of episodes to train for\n",
    "TRAINING_RUNS = 1 # Number of training runs\n",
    "NUM_GHOSTS = 2 # Number of ghosts present (2 simple, 3 in advanced)\n",
    "VISUALIZE = False # Option to visualise training\n",
    "MOVE_LIMIT = 800 # Maximum moves agent can make before episode truncation\n",
    "RANDOM = False # Random actions toggle\n",
    "DDQN = True # DDQN toggle (False reverts to DQN algorithm)\n",
    "NAIVE_BAYES = True # Naive Bayes shield active\n",
    "MULT = 0.5 # Data balancing multiplier\n",
    "THRESHOLD = 0.52 # Safe class probability threshold\n",
    "INITIAL_MAZE = FIREPIT_MAZE # Uncomment for Firepit maze usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a183093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DDQN results for comparison\n",
    "average_rewards, average_violations = import_results(\"Double_DQN_120k_3_runs.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c63e1b-8bad-4e98-84ca-1288ba9b14d0",
   "metadata": {},
   "source": [
    "#### Naive Bayes Shield training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Naive Bayes Shield training...\")\n",
    "t1 = time.time()\n",
    "all_rewards_NB = []\n",
    "all_violations_NB = []\n",
    "all_total_violations = 0\n",
    "\n",
    "for i in range(TRAINING_RUNS):\n",
    "    # Sum the runs and divide by number of runs to get the mean.\n",
    "    rewards_ddqn, total_violations, violations_per_episode_NB, route = run_episodes(NUM_EPISODES, NUM_GHOSTS, VISUALIZE, MOVE_LIMIT, RANDOM, DDQN, NAIVE_BAYES, MULT, THRESHOLD)\n",
    "    rew_100_ddqn = sum_rewards(rewards_ddqn, NUM_EPISODES)\n",
    "    all_rewards_NB.append(rew_100_ddqn)\n",
    "    all_violations_NB.append(violations_per_episode_NB)\n",
    "    all_total_violations += total_violations \n",
    "    \n",
    "average_rewards_NB = []\n",
    "average_violations_NB = []\n",
    "for i in range(NUM_EPISODES):\n",
    "    # Sum the runs and divide by number of runs to get the mean.\n",
    "    total = sum([j[i] for j in all_rewards_NB])\n",
    "    total_v = sum([j[i] for j in all_violations_NB])\n",
    "    average_rewards_NB.append(total/TRAINING_RUNS)\n",
    "    average_violations_NB.append(total_v/TRAINING_RUNS)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"***** Training Ended *****\")\n",
    "print(\"Average violations per run: \", all_total_violations/TRAINING_RUNS)\n",
    "print(f\"Naive Bayes with DDQN training time: {format_time(t2 - t1)}\")\n",
    "\n",
    "# Export results\n",
    "export_results(average_rewards_NB, average_violations_NB, \"Naive_Bayes_FIREPIT_2_runs_3_runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph of results.\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "# plt.plot(average_rewards, label=\"DQN\")\n",
    "plt.plot(average_rewards_NB, label=\"Shielded DDQN\") \n",
    "plt.ylim(0, 500)\n",
    "plt.xlim(0, NUM_EPISODES)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylabel('Average Reward over last 100 Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d791b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoomed in results\n",
    "#plt.plot(average_rewards, label=\"DDQN\")\n",
    "plt.plot(average_rewards_NB, label=\"Shielded DDQN\") \n",
    "plt.ylim(0, 500)\n",
    "plt.xlim((NUM_EPISODES/5*4), NUM_EPISODES)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.ylabel('Average Reward over last 100 Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9623113-8fc4-4c7a-8d90-87af8cb636e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards and violations\n",
    "figure, axis_1 = plt.subplots()\n",
    "#axis_1.plot(average_rewards, 'b', label='DDQN')\n",
    "axis_1.plot(average_rewards_NB, 'c--', label='Shielded')\n",
    "axis_1.set_xlabel('Episode')\n",
    "axis_1.set_ylabel('Average Reward over last 100 Episodes', color='b')\n",
    "axis_2 = axis_1.twinx()\n",
    "#axis_2.plot(cumulative_reward(average_violations), 'darkgreen', label='DQN')\n",
    "axis_2.plot(cumulative_reward(average_violations_NB), 'g--', label='Shielded DDQN')\n",
    "axis_2.set_ylabel('Cumulative Violations', color='g')\n",
    "axis_1.legend(loc='upper left')\n",
    "axis_2.legend(loc='lower right')\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7c348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards and violations with 1 in 1,000 points plotted.\n",
    "x = 1000\n",
    "figure, axis_1 = plt.subplots()\n",
    "#axis_1.plot(one_in_x(average_rewards, x), 'b', label='DDQN')\n",
    "axis_1.plot(one_in_x(average_rewards_NB, x), 'c--', label='Shielded DDQN')\n",
    "axis_1.set_xlabel('Episode')\n",
    "axis_1.set_ylabel('Average Reward over last 1000 Episodes', color='b')\n",
    "axis_2 = axis_1.twinx()\n",
    "#axis_2.plot(one_in_x(cumulative_reward(average_violations), x), 'darkgreen', label='DDQN')\n",
    "axis_2.plot(one_in_x(cumulative_reward(average_violations_NB), x), 'g--', label='Shielded DDQN')\n",
    "axis_2.set_ylabel('Cumulative Violations', color='g')\n",
    "axis_1.legend(loc='upper left')\n",
    "axis_2.legend(loc='lower right')\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdffca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add maze to route (most commonly taken actions at each coordinate)\n",
    "MAZE = copy.deepcopy(FIREPIT_MAZE)\n",
    "route_plus_maze = maze_and_route(FIREPIT_MAZE, route)\n",
    "\n",
    "# Visualise the combined maze and most common action grid\n",
    "visualize_maze(route_plus_maze, cell_size=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
